{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZ29daWzxRLL"
   },
   "source": [
    "# Building and Improving on LSTMs\n",
    "Lets build a long short term memory unit from scratch! For a clear and concise tutorial of LSTMs that heavily influenced this work, see [this writeup](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). The inner workings of LSTMs will also be explained throughout this notebook.\n",
    "\n",
    "Once we implement an a simple recurrent neural network (RNN) that makes use of LSTMs and test its performance, we will implement improvements based on the paper below titled \"Mogrifier LSTM\" and compare our results.\n",
    "\n",
    "\n",
    "## References\n",
    "[PyTorch LSTM Tutorial](https://mlexplained.com/2019/02/15/building-an-lstm-from-scratch-in-pytorch-lstms-in-depth-part-1/)\n",
    "\n",
    "[Understanding LSTMs writeup](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "[Video lesson on LSTMs from Andrew Ng](https://www.coursera.org/lecture/nlp-sequence-models/long-short-term-memory-lstm-KXoay)\n",
    "\n",
    "[Mogrifier LSTM](https://arxiv.org/abs/1909.01792)\n",
    "\n",
    "[LSTM Performance](https://github.com/sebastianruder/NLP-progress/blob/master/english/language_modeling.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHfykk4qlE0I"
   },
   "source": [
    "# RNNs\n",
    "In recurrent neural networks (RNNs), we generally want to maintain some memory of sequential data in order to make predictions. \n",
    ">![Image from wikipedia](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    ">*By FranÃ§ois Deloche - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=60109157*\n",
    "\n",
    "In the above diagram, the main thing to keep in mind is how memory is maintained. Inputs $x_t$ feed into hidden states $h_t$. Each cell (one set of red blue and green shapes) in the RNN (1) receives information from some part in a sequence of data, (2) reads it into the hidden state, and (3) transforms that information and passes it on to the next hidden state in the chain. In this manner, past data is fed through the entire network in case it might be useful to some cell later in the chain.\n",
    "\n",
    "The problem with this approach to RNNs is that we generally run into the problem of vanishing gradients. What this means is that our data from past hidden states eventually gets multiplied to be so small as to be neglible. Our ability to remember things from even a few states ago is not so great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRmOAFDYlE0J"
   },
   "source": [
    "# Enter LSTMs\n",
    "To solve the problem of vanishing gradients, the idea of the long short term memory (LSTM) cell was created. This cell replaces the RNN cell referred to above in order to alleviate some issues with RNNs and acheive better performance on sequential data.\n",
    "\n",
    "## But how do LSTMs work?\n",
    "\n",
    "Essentially what LSTMs do is expand on the basic RNN cell by adding a parallel branch that tracks data that happened further in the past. In this way, we can avoid the vanishing gradient issue of basic RNN cells.\n",
    "The below diagram shows the flow of information in an LSTM cell.\n",
    "\n",
    ">![image](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/1920px-The_LSTM_cell.png)\n",
    ">By Guillaume Chevalier - Own work, CC BY 4.0, https://commons.wikimedia.org/w/index.php?curid=71836793\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhLBgM7clE0J"
   },
   "source": [
    "The equations for the LSTM cell look like this (taken from [here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)):\n",
    "\n",
    "$\\begin{array}{ll} \\\\\n",
    "  f_t = \\sigma(W_{f} [x_t,h_{t-1}] + b_{f}) \\\\\n",
    "  i_t = \\sigma(W_{i} [x_t,h_{t-1}] + b_{i}) \\\\\n",
    "  \\tilde{C_t} = \\tanh(W_{ig} [x_t,h_{t-1}] + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "  o_t = \\sigma(W_{o} [x_t,h_{t-1}] + b_{o}) \\\\\n",
    "  C_t = f_t * C_{t-1} + i_t * \\tilde{C_t} \\\\\n",
    "  h_t = o_t * \\tanh(C_t) \\\\\n",
    "\\end{array}$\n",
    "\n",
    "*Note that $[x_t,h_{t-1}]$ refers to the concatenation of the $x_t$ and $h_{t-1}$ matrices. Some variations of LSTM implementations split these instead. Depending on the implementation, this may lead to some lost information.*\n",
    "\n",
    "$f_t$ is the function for the forget gate.\n",
    "\n",
    "$i_t$ is the function for the input gate.\n",
    "\n",
    "$\\tilde{C_t}$ is the function for the candidate memory cell update.\n",
    "\n",
    "$o_t$ is an intermediate calculation for determining the hidden state $h_t$.\n",
    "\n",
    "$C_t$ is the function for the updated memory cell.\n",
    "\n",
    "$h_t$ is the function for the update hidden state.\n",
    "\n",
    "**While initially daunting**, what is happening here is not too complicated. Essentially, we have two gates, one for determining whether we will forget ($f_t$) some part of our memory cell $C$ and an input/update gate for determining whether we will add new information into our memory cell ($i_t$). Finally, we update our hidden state $h$ based on whatever information is currently in the memory cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Mdu2qsFlE0b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "\n",
    "from typing import *\n",
    "from pathlib import Path\n",
    "DATA_ROOT = Path(\"../data/brown\")\n",
    "N_EPOCHS = 210\n",
    "from enum import IntEnum\n",
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGp-5a9PlE0a"
   },
   "source": [
    "# Implementing the LSTM\n",
    "\n",
    "Given what we've learned, lets implement our own LSTM in PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run this cell if running the notebook from Google Colaboratory\n",
    "!pip install allennlp\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_c7huC6QlE0d",
    "outputId": "7307a3b2-938a-4557-a511-9718ed4e7561"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 16])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NaiveLSTM(nn.Module):\n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        #Define/initialize all tensors   \n",
    "        # forget gate\n",
    "        self.Wf = Parameter(torch.Tensor(input_sz+hidden_sz, hidden_sz))\n",
    "        self.bf = Parameter(torch.Tensor(hidden_sz))\n",
    "        # input gate\n",
    "        self.Wi = Parameter(torch.Tensor(input_sz+hidden_sz, hidden_sz))\n",
    "        self.bi = Parameter(torch.Tensor(hidden_sz))\n",
    "        # Candidate memory cell\n",
    "        self.Wc = Parameter(torch.Tensor(input_sz+hidden_sz, hidden_sz))\n",
    "        self.bc = Parameter(torch.Tensor(hidden_sz))\n",
    "        # output gate\n",
    "        self.Wo = Parameter(torch.Tensor(input_sz+hidden_sz, hidden_sz))\n",
    "        self.bo = Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                nn.init.zeros_(p.data)\n",
    "        \n",
    "    #Define forward pass through all LSTM cells across all timesteps.\n",
    "    #By using PyTorch functions, we get backpropagation for free.\n",
    "    def forward(self, x: torch.Tensor, \n",
    "                init_states: Optional[Tuple[torch.Tensor, torch.Tensor]]=None\n",
    "               ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        batch_sz, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        #ht and Ct start as the previous states and end as the output states in each loop bellow\n",
    "        if init_states is None:\n",
    "            ht = torch.zeros((batch_sz,self.hidden_size)).to(x.device)\n",
    "            Ct = torch.zeros((batch_sz,self.hidden_size)).to(x.device)\n",
    "        else:\n",
    "            ht, Ct = init_states\n",
    "        for t in range(seq_sz): # iterate over the time steps\n",
    "            xt = x[:, t, :]\n",
    "            hx_concat = torch.cat((ht,xt),dim=1)\n",
    "\n",
    "            ### The LSTM Cell!\n",
    "            ft = torch.sigmoid(hx_concat @ self.Wf + self.bf)\n",
    "            it = torch.sigmoid(hx_concat @ self.Wi + self.bi)\n",
    "            Ct_candidate = torch.tanh(hx_concat @ self.Wc + self.bc)\n",
    "            ot = torch.sigmoid(hx_concat @ self.Wo + self.bo)\n",
    "            #outputs\n",
    "            Ct = ft * Ct + it * Ct_candidate\n",
    "            ht = ot * torch.tanh(Ct)\n",
    "            ###\n",
    "\n",
    "            hidden_seq.append(ht.unsqueeze(Dim.batch))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=Dim.batch)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(Dim.batch, Dim.seq).contiguous()\n",
    "        return hidden_seq, (ht, Ct)\n",
    "\n",
    "#sanity testing\n",
    "#note that our hidden_sz is also our defined output size for each LSTM cell.\n",
    "batch_sz, seq_len, feat_sz, hidden_sz = 5, 10, 32, 16\n",
    "arr = torch.randn(batch_sz, seq_len, feat_sz)\n",
    "lstm = NaiveLSTM(feat_sz, hidden_sz)\n",
    "ht, (hn, cn) = lstm(arr)\n",
    "ht.shape #shape should be batch_sz x seq_len x hidden_sz = 5x10x16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSzGsmxplE0p"
   },
   "source": [
    "# Testing the Implementation\n",
    "Now, that we've covered the basics and have a minimally working LSTM, we'll put our model into action. Our testbed will be a character-level language modeling task. We'll be using the Brown Corpus which you can get via the commands below.\n",
    "\n",
    ">More information on the Brown corpus can be found [here](https://en.wikipedia.org/wiki/Brown_Corpus).\n",
    "\n",
    ">\"The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) was compiled in the 1960s by Henry KuÄera and W. Nelson Francis at Brown University, Providence, Rhode Island as a general corpus (text collection) in the field of corpus linguistics. It contains 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "DTBnkZK1lE0q",
    "outputId": "fb0393f8-16fd-4881-b637-7df1555a834e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 6040k  100 6040k    0     0  5687k      0  0:00:01  0:00:01 --:--:-- 5687k\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p {DATA_ROOT}\n",
    "!curl http://www.sls.hawaii.edu/bley-vroman/brown.txt -o {DATA_ROOT / \"brown.txt\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCB38uxdlE0t"
   },
   "source": [
    "We'll let AllenNLP--an NLP library made to simplify training in PyTorch--handle the complexity of training the language model and building up the datasets. What's happening below is we are tokenizing the characters in the dataset and then splitting the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "UmCw9G_ulE0v",
    "outputId": "43f8a410-68e7-42cc-8323-16583c398122"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/11994 [00:00<?, ?it/s]\u001b[A\n",
      "100%|ââââââââââ| 11994/11994 [00:00<00:00, 63342.18it/s][A\n",
      "11994it [00:06, 1890.08it/s]\n",
      "100%|ââââââââââ| 10794/10794 [00:03<00:00, 3346.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.dataset_readers import LanguageModelingReader\n",
    "from allennlp.data.tokenizers import CharacterTokenizer\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.iterators import BasicIterator\n",
    "from allennlp.training import Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "char_tokenizer = CharacterTokenizer(lowercase_characters=True)\n",
    "\n",
    "reader = LanguageModelingReader(\n",
    "    tokens_per_instance=500,\n",
    "    tokenizer=char_tokenizer,\n",
    "    token_indexers = {\"tokens\": SingleIdTokenIndexer()},\n",
    ")\n",
    "\n",
    "train_ds = reader.read(DATA_ROOT / \"brown.txt\")\n",
    "train_ds, val_ds = train_test_split(train_ds, random_state=0, test_size=0.1)\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_ds)\n",
    "\n",
    "iterator = BasicIterator(batch_size=32)\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MbtyvFIATNM"
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module, epochs: int,log_dir):\n",
    "    trainer = Trainer( patience=7,\n",
    "        histogram_interval=10,\n",
    "        summary_interval= 10,\n",
    "        serialization_dir=log_dir,\n",
    "        model=model.cuda() if torch.cuda.is_available() else model,\n",
    "        optimizer=optim.Adam(model.parameters()),\n",
    "        iterator=iterator, train_dataset=train_ds, \n",
    "        validation_dataset=val_ds, num_epochs=epochs,\n",
    "        cuda_device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fB9eEFoT1GMN"
   },
   "source": [
    "We build our NLP neural network below using 3 layers:\n",
    "\n",
    "1.   An embedding layer\n",
    "2.   An encoding layer (a set of our LSTM cells based on the sequence size)\n",
    "3.   A projection layer (to convert the text back out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdVfTb4OlE0x"
   },
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.models import Model\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "\n",
    "class LanguageModel(Model):\n",
    "    def __init__(self, encoder: nn.RNN, vocab: Vocabulary,\n",
    "                 embedding_dim: int=50):\n",
    "        super().__init__(vocab=vocab)\n",
    "        # char embedding\n",
    "        self.vocab_size = vocab.get_vocab_size()\n",
    "        self.padding_idx = vocab.get_token_index(\"@@PADDING@@\")\n",
    "        token_embedding = Embedding(\n",
    "            num_embeddings=vocab.get_vocab_size(),\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_index=self.padding_idx,\n",
    "        )\n",
    "        self.embedding = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "        self.encoder = encoder\n",
    "        self.projection = nn.Linear(self.encoder.hidden_size, self.vocab_size)\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=self.padding_idx)\n",
    "    \n",
    "    def forward(self, input_tokens: Dict[str, torch.Tensor],\n",
    "                output_tokens: Dict[str, torch.Tensor]):\n",
    "        embs = self.embedding(input_tokens)\n",
    "        x, _ = self.encoder(embs)\n",
    "        x = self.projection(x)\n",
    "        if output_tokens is not None:\n",
    "            loss = self.loss(x.view((-1, self.vocab_size)), output_tokens[\"tokens\"].flatten())\n",
    "        else:\n",
    "            loss = None\n",
    "        return {\"loss\": loss, \"logits\": x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVO3DUOOlE0z"
   },
   "source": [
    "Now, let's try training. We'll just do it for one epoch to verify our LSTM cell works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "stN7plj9lE0z",
    "outputId": "a98cbf1b-077f-4936-c763-15bcaba9b0aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.7181 ||: 100%|ââââââââââ| 338/338 [02:22<00:00,  2.38it/s]\n",
      "loss: 2.3362 ||: 100%|ââââââââââ| 38/38 [00:05<00:00,  6.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 0,\n",
       " 'peak_cpu_memory_MB': 3972.904,\n",
       " 'peak_gpu_0_memory_MB': 1448,\n",
       " 'training_duration': '0:02:27.938562',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 0,\n",
       " 'epoch': 0,\n",
       " 'training_loss': 2.7181027852571926,\n",
       " 'training_cpu_memory_MB': 3972.904,\n",
       " 'training_gpu_0_memory_MB': 1448,\n",
       " 'validation_loss': 2.336242989489907,\n",
       " 'best_validation_loss': 2.336242989489907}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_naive = LanguageModel(NaiveLSTM(50, 125), vocab)\n",
    "LSTM_trainer = train(lm_naive,1,\"./run/lstm\")\n",
    "LSTM_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LR7A7BZglE03"
   },
   "source": [
    "Now, let's compare with the official LSTM. We'll do this one until it looks like the loss is no longer decreasing so we can see what kind of accuracy this model is capable of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "1ATRWUGHlE03",
    "outputId": "b4127467-ea9b-49ed-bb65-147fc74231c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.6821 ||: 100%|ââââââââââ| 338/338 [00:12<00:00, 26.91it/s]\n",
      "loss: 2.3122 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 61.74it/s]\n",
      "loss: 2.1900 ||: 100%|ââââââââââ| 338/338 [00:12<00:00, 26.86it/s]\n",
      "loss: 2.0937 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 64.30it/s]\n",
      "loss: 2.0334 ||: 100%|ââââââââââ| 338/338 [00:12<00:00, 26.64it/s]\n",
      "loss: 1.9765 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 64.47it/s]\n",
      "loss: 1.9377 ||: 100%|ââââââââââ| 338/338 [00:12<00:00, 26.29it/s]\n",
      "loss: 1.8980 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 51.71it/s]\n",
      "loss: 1.8707 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 25.20it/s]\n",
      "loss: 1.8400 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 58.74it/s]\n",
      "loss: 1.8186 ||: 100%|ââââââââââ| 338/338 [00:12<00:00, 26.01it/s]\n",
      "loss: 1.7933 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 63.21it/s]\n",
      "loss: 1.7754 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 25.80it/s]\n",
      "loss: 1.7533 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 50.37it/s]\n",
      "loss: 1.7387 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 25.04it/s]\n",
      "loss: 1.7200 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 65.45it/s]\n",
      "loss: 1.7079 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 25.81it/s]\n",
      "loss: 1.6923 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 65.10it/s]\n",
      "loss: 1.6822 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 25.61it/s]\n",
      "loss: 1.6690 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 55.69it/s]\n",
      "loss: 1.6601 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 25.95it/s]\n",
      "loss: 1.6493 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 59.54it/s]\n",
      "loss: 1.6415 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.83it/s]\n",
      "loss: 1.6319 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 58.37it/s]\n",
      "loss: 1.6251 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 25.31it/s]\n",
      "loss: 1.6170 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 56.83it/s]\n",
      "loss: 1.6109 ||: 100%|ââââââââââ| 338/338 [00:14<00:00, 24.04it/s]\n",
      "loss: 1.6033 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 45.15it/s]\n",
      "loss: 1.5977 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.29it/s]\n",
      "loss: 1.5919 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 58.08it/s]\n",
      "loss: 1.5862 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.17it/s]\n",
      "loss: 1.5810 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 61.38it/s]\n",
      "loss: 1.5756 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.32it/s]\n",
      "loss: 1.5707 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 55.13it/s]\n",
      "loss: 1.5663 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.78it/s]\n",
      "loss: 1.5622 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 63.08it/s]\n",
      "loss: 1.5574 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.49it/s]\n",
      "loss: 1.5540 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 45.97it/s]\n",
      "loss: 1.5499 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.39it/s]\n",
      "loss: 1.5474 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 56.22it/s]\n",
      "loss: 1.5426 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.17it/s]\n",
      "loss: 1.5415 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 56.50it/s]\n",
      "loss: 1.5360 ||: 100%|ââââââââââ| 338/338 [00:14<00:00, 24.02it/s]\n",
      "loss: 1.5340 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 58.27it/s]\n",
      "loss: 1.5299 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 25.18it/s]\n",
      "loss: 1.5290 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 46.36it/s]\n",
      "loss: 1.5242 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.89it/s]\n",
      "loss: 1.5232 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 61.94it/s]\n",
      "loss: 1.5190 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 25.28it/s]\n",
      "loss: 1.5193 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 41.69it/s]\n",
      "loss: 1.5140 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 25.18it/s]\n",
      "loss: 1.5146 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 60.45it/s]\n",
      "loss: 1.5092 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.39it/s]\n",
      "loss: 1.5096 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 42.72it/s]\n",
      "loss: 1.5049 ||: 100%|ââââââââââ| 338/338 [00:14<00:00, 24.11it/s]\n",
      "loss: 1.5060 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 61.35it/s]\n",
      "loss: 1.5008 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.36it/s]\n",
      "loss: 1.5026 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 57.32it/s]\n",
      "loss: 1.4970 ||: 100%|ââââââââââ| 338/338 [00:14<00:00, 23.69it/s]\n",
      "loss: 1.5001 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 52.50it/s]\n",
      "loss: 1.4933 ||: 100%|ââââââââââ| 338/338 [00:14<00:00, 23.88it/s]\n",
      "loss: 1.4963 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 58.10it/s]\n",
      "loss: 1.4900 ||: 100%|ââââââââââ| 338/338 [00:14<00:00, 24.04it/s]\n",
      "loss: 1.4925 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 57.26it/s]\n",
      "loss: 1.4864 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.37it/s]\n",
      "loss: 1.4895 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 55.32it/s]\n",
      "loss: 1.4838 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.17it/s]\n",
      "loss: 1.4878 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 48.44it/s]\n",
      "loss: 1.4804 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.26it/s]\n",
      "loss: 1.4839 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 52.28it/s]\n",
      "loss: 1.4778 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.62it/s]\n",
      "loss: 1.4822 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 56.49it/s]\n",
      "loss: 1.4752 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 25.03it/s]\n",
      "loss: 1.4794 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 44.85it/s]\n",
      "loss: 1.4726 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.95it/s]\n",
      "loss: 1.4770 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 43.70it/s]\n",
      "loss: 1.4705 ||: 100%|ââââââââââ| 338/338 [00:14<00:00, 23.44it/s]\n",
      "loss: 1.4755 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 48.16it/s]\n",
      "loss: 1.4680 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.18it/s]\n",
      "loss: 1.4735 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 50.66it/s]\n",
      "loss: 1.4658 ||: 100%|ââââââââââ| 338/338 [00:14<00:00, 23.89it/s]\n",
      "loss: 1.4713 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 54.46it/s]\n",
      "loss: 1.4636 ||: 100%|ââââââââââ| 338/338 [00:13<00:00, 24.18it/s]\n",
      "loss: 1.4701 ||: 100%|ââââââââââ| 38/38 [00:00<00:00, 59.49it/s]\n",
      "loss: 1.4643 ||:  35%|ââââ      | 117/338 [00:04<00:09, 23.79it/s]"
     ]
    }
   ],
   "source": [
    "lm_comparison = LanguageModel(nn.LSTM(50, 125, batch_first=True), vocab)\n",
    "official_LSTM = train(lm_comparison, N_EPOCHS,\"./run/officiallstm\")\n",
    "official_LSTM.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TW0cc_YJdJqL"
   },
   "source": [
    "It looks like our basic implementation without any optimizations is a little over 100x slower than the built in one (185 seconds vs 15 seconds in my testing) while the number of iterations it takes to acheive similar accuracy is about the same. We could investigate making further improvements to ours (more efficient batches, fewer calculations, etc.), but lets leave it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EDip4uF3lE06"
   },
   "source": [
    "# Mogrifier LSTM!\n",
    "\n",
    "Lets implement a version of the Mogrifier LSTM based on [this paper](https://arxiv.org/abs/1909.01792).\n",
    "\n",
    ">![mogrifier](https://drive.google.com/uc?id=1yMgPjXW_SV29Y-uuJsKNdknhm5CylWAu)\n",
    "\n",
    "Essentially what this paper does is provide another gate prior to the input into each LSTM cell that is entirely based on the interaction between the hidden state and the input. Read the paper if you'd like the intuition behind it.\n",
    "\n",
    "Lets look at the equations we need to implement:\n",
    "\n",
    "$x^i = 2\\sigma(Q^ih^{i-1}_{prev}) * x^{i-2}$ for odd $i \\in [1...r]$\n",
    "\n",
    "$h^i_{prev} = 2\\sigma(R^ix^{i-1}) * h^{i-2}_{prev}$ for even $i \\in [1...r]$\n",
    "\n",
    "So all we have to do is add some randomly initialized weights $Q$ and $R$ that will gate the inputs $x_t$ and $h_{t-1}$ in alternating fashion. Lets see how it compares!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TvlXNi22Jwkh",
    "outputId": "38d66788-9c8c-4a3a-be62-e97353d5a2b2"
   },
   "outputs": [],
   "source": [
    "class MogLSTM(nn.Module):\n",
    "    def __init__(self, input_sz: int, hidden_sz: int, mog_iterations: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        self.mog_iterations = mog_iterations\n",
    "        #Define/initialize all tensors   \n",
    "        self.Wih = Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n",
    "        self.Whh = Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n",
    "        self.bih = Parameter(torch.Tensor(hidden_sz * 4))\n",
    "        self.bhh = Parameter(torch.Tensor(hidden_sz * 4))\n",
    "        #Mogrifiers\n",
    "        self.Q = Parameter(torch.Tensor(hidden_sz,input_sz))\n",
    "        self.R = Parameter(torch.Tensor(input_sz,hidden_sz))\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                nn.init.zeros_(p.data)\n",
    "\n",
    "    def mogrify(self,xt,ht):\n",
    "      for i in range(1,self.mog_iterations+1):\n",
    "        if (i % 2 == 0):\n",
    "          ht = (2*torch.sigmoid(xt @ self.R)) * ht\n",
    "        else:\n",
    "          xt = (2*torch.sigmoid(ht @ self.Q)) * xt\n",
    "\n",
    "    \n",
    "    #Define forward pass through all LSTM cells across all timesteps.\n",
    "    #By using PyTorch functions, we get backpropagation for free.\n",
    "    def forward(self, x: torch.Tensor, \n",
    "                init_states: Optional[Tuple[torch.Tensor, torch.Tensor]]=None\n",
    "               ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        batch_sz, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        #ht and Ct start as the previous states and end as the output states in each loop below\n",
    "        if init_states is None:\n",
    "            ht = torch.zeros((batch_sz,self.hidden_size)).to(x.device)\n",
    "            Ct = torch.zeros((batch_sz,self.hidden_size)).to(x.device)\n",
    "        else:\n",
    "            ht, Ct = init_states\n",
    "        for t in range(seq_sz): # iterate over the time steps\n",
    "            xt = x[:, t, :]\n",
    "            self.mogrify(xt,ht)\n",
    "            gates = (xt @ self.Wih + self.bih) + (ht @ self.Whh + self.bhh)\n",
    "            ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "            ### The LSTM Cell!\n",
    "            ft = torch.sigmoid(forgetgate)\n",
    "            it = torch.sigmoid(ingate)\n",
    "            Ct_candidate = torch.tanh(cellgate)\n",
    "            ot = torch.sigmoid(outgate)\n",
    "            #outputs\n",
    "            Ct = (ft * Ct) + (it * Ct_candidate)\n",
    "            ht = ot * torch.tanh(Ct)\n",
    "            ###\n",
    "\n",
    "            hidden_seq.append(ht.unsqueeze(Dim.batch))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=Dim.batch)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(Dim.batch, Dim.seq).contiguous()\n",
    "        return hidden_seq, (ht, Ct)\n",
    "\n",
    "#sanity testing\n",
    "#note that our hidden_sz is also our defined output size for each LSTM cell.\n",
    "batch_sz, seq_len, feat_sz, hidden_sz = 5, 10, 32, 16\n",
    "arr = torch.randn(batch_sz, seq_len, feat_sz)\n",
    "lstm = NaiveLSTM(feat_sz, hidden_sz)\n",
    "ht, (hn, cn) = lstm(arr)\n",
    "ht.shape #shape should be batch_sz x seq_len x hidden_sz = 5x10x16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "3qCTcuxE5rSS",
    "outputId": "1d89fc72-3a04-4673-a1cc-d2c865cb64bc"
   },
   "outputs": [],
   "source": [
    "lm_mog = LanguageModel(MogLSTM(50, 125,5), vocab)\n",
    "mog_LSTM = train(lm_mog, 45, \"./run/mog2\")\n",
    "mog_LSTM.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ug9IzRBnX3oe"
   },
   "source": [
    "\n",
    "Based on our loss values, it looks like our accuracy may be better than the standard LSTM (still needs to train for more epochs though--likely will want to do this on a local machine). The Mogrifier LSTM makes no strong claims regarding increased speed (though they do claim lower complexity, which probably means they optimized much more than me), but they did indicate it acheives better accuracy. It's hard to say for sure without doing more testing though. \n",
    "\n",
    "The writers of the Mogrifier LSTM paper have stated they will release their code, but so far this has not ocurred. We can try a few tweaks (different randomization, different test model, other datasets, different test methods, more training) to better verify what's going on, but we'll leave it at this for now and look for the code release to see if there's anything else that can be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jSUQ-GkWlE1C"
   },
   "source": [
    "# Visualizing It\n",
    "\n",
    "Now that we've implemented a few things, lets go ahead and visualize it all. We're using tensorboard since it does a bunch of plotting work for us. The one downside at the moment is that AllenNLP's training class does not give us the option of comparing graphs from different models, so for now we'll look at them separately. \n",
    "\n",
    "**Note: This visualization requires tensorboard, which you may not have installed. It is intended for use on Google Colaboratory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_pg2sQYlE1C"
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "%tensorboard --logdir \"./run/mog2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If on Google Colaboratory you can save and export all the data we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ETILSLErur6_",
    "outputId": "41b66e4d-4648-4051-f284-9e7370c2689f"
   },
   "outputs": [],
   "source": [
    "#zip results\n",
    "!zip -r all.zip ./run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "zKuAXXpI2whz",
    "outputId": "26780ccf-55aa-4043-962c-d65fb3b93166"
   },
   "outputs": [],
   "source": [
    "#download/upload Colaboratory files to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjzGX8TR5UAt"
   },
   "outputs": [],
   "source": [
    "!cp all.zip /content/drive/My\\ Drive/all.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnguGfZMFlFo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_from_scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
